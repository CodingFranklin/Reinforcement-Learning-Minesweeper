# Reinforcement-Learning-Minesweeper

## Files

 - **Settings.py**

Central configuration file for the entire project. This file defines the board size (ROWS, COLS) and the number of mines (AMOUNT_OF_MINES), which directly control the difficulty of the game. It also contains rendering-related settings such as tile size, window width/height, frame rate, and window title. Changing ROWS or COLS affects the observation size and action space, so agents must be retrained after modifying these values. This is the main place to adjust board size and mine count.

 - **Sprites.py**

Implements the core Minesweeper game logic and visual components. It defines the Tile class, which represents a single cell and stores its type, revealed state, flag state, and clue value, and the Board class, which manages mine placement, clue calculation, recursive empty expansion, and digging logic. Mines are placed on the first dig to ensure a safe initial move. This file also handles drawing tiles to the screen for the pygame interface.

 - **Game.py**

By running this, you can play the original Minesweeper.

 - **Env_Minesweeper.py**

Wraps the Minesweeper game into a reinforcement learning environment. It defines the reset and step functions, encodes the board state into a numerical observation matrix, and determines episode termination. This file also defines the reward function, including penalties for hitting a mine or invalid actions and bonuses for winning or correct flag placement. Reward shaping and environment behavior should be adjusted here.

 - **Random_Agent.py**

Implements a baseline random agent that only performs dig actions. The number of training episodes and maximum steps per episode can be adjusted using NUM_EPISODES and MAX_STEPS. Episode rewards are recorded and saved to a JSON file for later comparison with learning-based agents.

 - **Random_Agent_Flag.py**

A variant of the random agent that can perform both dig and flag actions. The action space is doubled to include flag operations. Like the basic random agent, it uses fixed episode and step limits and saves reward data for evaluation.

 - **DQN_Agent.py**

Implements a Deep Q-Network agent that learns to dig tiles only. The neural network maps the board observation to one Q-value per tile. This file contains most training hyperparameters, including the number of episodes, learning rate, discount factor, batch size, replay buffer size, and epsilon-greedy exploration schedule. Adjust this file to change training length, exploration behavior, or network learning speed.

 - **DQN_Agent_Flag.py**

Extends the DQN agent to include both dig and flag actions by doubling the action space. It uses a similar network architecture but includes a different epsilon decay strategy and a configurable target network update interval. Training duration, exploration parameters, and learning rate can all be modified here.

 - **plot_agents.py**

Loads reward data generated by the random and DQN agents and plots training curves for comparison. It supports optional smoothing and allows selective enabling or disabling of different agent curves. This file is used purely for visualization and analysis of training performance. Be sure to run both DQN agent and random agent before plotting the graph.

## Settings

**'Board size' and mine count (Settings.py)**

The board dimensions and difficulty are controlled by ROWS, COLS, and AMOUNT_OF_MINES. Changing these values adjusts the size of the Minesweeper grid and the number of mines placed on the board. Modifying ROWS or COLS changes the observation size and action space, so all agents must be retrained after such changes.

**Rendering and game display (Settings.py)**

Visual and runtime settings such as TILESIZE, WIDTH, HEIGHT, FPS, and TITLE determine the appearance and frame rate of the human-playable Minesweeper game. These settings do not affect reinforcement learning performance unless rendering is enabled during training.

**Reward design and environment behavior (Env_Minesweeper.py)**

Rewards and penalties are defined by constants such as MINE_PENALTY, WIN_BONUS, FLAG_REWARD_CORRECT, FLAG_REWARD_WRONG, and INVALID_ACTION_PENALTY. Adjusting these values changes how the agent is encouraged or discouraged to dig, flag, or avoid invalid actions, and has a major impact on learning stability.

**Training duration (DQN_Agent.py, DQN_Agent_Flag.py, Random_Agent.py)**

The total number of training episodes and the maximum number of steps per episode are controlled by variables such as EPISODES or NUM_EPISODES and MAX_STEPS. Increasing these values allows longer training but also increases computation time.

**Exploration strategy (DQN_Agent.py, DQN_Agent_Flag.py)**

Epsilon-greedy exploration parameters, including EPS_START, EPS_END, EPS_DECAY, or episode-based decay schedules, control how much the agent explores random actions versus exploiting learned policies during training.

**Learning hyperparameters (DQN_Agent.py, DQN_Agent_Flag.py)**

Core learning behavior is governed by parameters such as the learning rate (LR), discount factor (GAMMA), batch size (BATCH_SIZE), replay buffer size (REPLAY_SIZE or REPLAY_CAPACITY), and target network update frequency. These values influence convergence speed, stability, and final performance.

**Action space definition (Env_Minesweeper.py, agent files)**

Whether the agent can only dig tiles or both dig and place flags is determined by how the action space is defined. Dig-only agents use ROWS * COLS actions, while dig-and-flag agents use ROWS * COLS * 2, which must be consistent between the environment and the agent.

**Result visualization (plot_agents.py)**

Plotting behavior, including which agents are compared and how much smoothing is applied to reward curves, can be adjusted in plot_agents.py. This affects only result presentation and not training itself.
