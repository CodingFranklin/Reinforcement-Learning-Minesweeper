# Reinforcement-Learning-Minesweeper

 - **Settings.py**

Central configuration file for the entire project. This file defines the board size (ROWS, COLS) and the number of mines (AMOUNT_OF_MINES), which directly control the difficulty of the game. It also contains rendering-related settings such as tile size, window width/height, frame rate, and window title. Changing ROWS or COLS affects the observation size and action space, so agents must be retrained after modifying these values. This is the main place to adjust board size and mine count.

 - **Sprites.py**

Implements the core Minesweeper game logic and visual components. It defines the Tile class, which represents a single cell and stores its type, revealed state, flag state, and clue value, and the Board class, which manages mine placement, clue calculation, recursive empty expansion, and digging logic. Mines are placed on the first dig to ensure a safe initial move. This file also handles drawing tiles to the screen for the pygame interface.

 - **Game.py**

By running this, you can play the original Minesweeper.

 - **Env_Minesweeper.py**

Wraps the Minesweeper game into a reinforcement learning environment. It defines the reset and step functions, encodes the board state into a numerical observation matrix, and determines episode termination. This file also defines the reward function, including penalties for hitting a mine or invalid actions and bonuses for winning or correct flag placement. Reward shaping and environment behavior should be adjusted here.

 - **Random_Agent.py**

Implements a baseline random agent that only performs dig actions. The number of training episodes and maximum steps per episode can be adjusted using NUM_EPISODES and MAX_STEPS. Episode rewards are recorded and saved to a JSON file for later comparison with learning-based agents.

 - **Random_Agent_Flag.py**

A variant of the random agent that can perform both dig and flag actions. The action space is doubled to include flag operations. Like the basic random agent, it uses fixed episode and step limits and saves reward data for evaluation.

 - **DQN_Agent.py**

Implements a Deep Q-Network agent that learns to dig tiles only. The neural network maps the board observation to one Q-value per tile. This file contains most training hyperparameters, including the number of episodes, learning rate, discount factor, batch size, replay buffer size, and epsilon-greedy exploration schedule. Adjust this file to change training length, exploration behavior, or network learning speed.

 - **DQN_Agent_Flag.py**

Extends the DQN agent to include both dig and flag actions by doubling the action space. It uses a similar network architecture but includes a different epsilon decay strategy and a configurable target network update interval. Training duration, exploration parameters, and learning rate can all be modified here.

 - **plot_agents.py**

Loads reward data generated by the random and DQN agents and plots training curves for comparison. It supports optional smoothing and allows selective enabling or disabling of different agent curves. This file is used purely for visualization and analysis of training performance. Be sure to run both DQN agent and random agent before plotting the graph.
